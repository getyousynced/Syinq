import * as pulumi from "@pulumi/pulumi";
import * as inputs from "./types/input";
import * as outputs from "./types/output";
/**
 * ## Example Usage
 *
 * ## Import
 *
 * Import an account-scoped job.
 *
 * ```sh
 * $ pulumi import cloudflare:index/logpushJob:LogpushJob example account/<account_id>/<job_id>
 * ```
 *
 * Import a zone-scoped job.
 *
 * ```sh
 * $ pulumi import cloudflare:index/logpushJob:LogpushJob example zone/<zone_id>/<job_id>
 * ```
 */
export declare class LogpushJob extends pulumi.CustomResource {
    /**
     * Get an existing LogpushJob resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    static get(name: string, id: pulumi.Input<pulumi.ID>, state?: LogpushJobState, opts?: pulumi.CustomResourceOptions): LogpushJob;
    /**
     * Returns true if the given object is an instance of LogpushJob.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    static isInstance(obj: any): obj is LogpushJob;
    /**
     * The account identifier to target for the resource. Must provide only one of `accountId`, `zoneId`.
     */
    readonly accountId: pulumi.Output<string | undefined>;
    /**
     * The kind of the dataset to use with the logpush job. Available values: `accessRequests`, `casbFindings`, `firewallEvents`, `httpRequests`, `spectrumEvents`, `nelReports`, `auditLogs`, `gatewayDns`, `gatewayHttp`, `gatewayNetwork`, `dnsLogs`, `networkAnalyticsLogs`, `workersTraceEvents`, `devicePostureResults`, `zeroTrustNetworkSessions`, `magicIdsDetections`, `pageShieldEvents`.
     */
    readonly dataset: pulumi.Output<string>;
    /**
     * Uniquely identifies a resource (such as an s3 bucket) where data will be pushed. Additional configuration parameters supported by the destination may be included. See [Logpush destination documentation](https://developers.cloudflare.com/logs/reference/logpush-api-configuration#destination).
     */
    readonly destinationConf: pulumi.Output<string>;
    /**
     * Whether to enable the job.
     */
    readonly enabled: pulumi.Output<boolean | undefined>;
    /**
     * Use filters to select the events to include and/or remove from your logs. For more information, refer to [Filters](https://developers.cloudflare.com/logs/reference/logpush-api-configuration/filters/).
     */
    readonly filter: pulumi.Output<string | undefined>;
    /**
     * A higher frequency will result in logs being pushed on faster with smaller files. `low` frequency will push logs less often with larger files. Available values: `high`, `low`. Defaults to `high`.
     */
    readonly frequency: pulumi.Output<string | undefined>;
    /**
     * The kind of logpush job to create. Available values: `edge`, `instant-logs`, `""`.
     */
    readonly kind: pulumi.Output<string | undefined>;
    /**
     * Configuration string for the Logshare API. It specifies things like requested fields and timestamp formats. See [Logpush options documentation](https://developers.cloudflare.com/logs/logpush/logpush-configuration-api/understanding-logpush-api/#options).
     */
    readonly logpullOptions: pulumi.Output<string | undefined>;
    /**
     * The maximum uncompressed file size of a batch of logs. Value must be between 5MB and 1GB.
     */
    readonly maxUploadBytes: pulumi.Output<number | undefined>;
    /**
     * The maximum interval in seconds for log batches. Value must be between 30 and 300.
     */
    readonly maxUploadIntervalSeconds: pulumi.Output<number | undefined>;
    /**
     * The maximum number of log lines per batch. Value must be between 1000 and 1,000,000.
     */
    readonly maxUploadRecords: pulumi.Output<number | undefined>;
    /**
     * The name of the logpush job to create.
     */
    readonly name: pulumi.Output<string | undefined>;
    /**
     * Structured replacement for logpull*options. When including this field, the logpull*option field will be ignored.
     */
    readonly outputOptions: pulumi.Output<outputs.LogpushJobOutputOptions | undefined>;
    /**
     * Ownership challenge token to prove destination ownership, required when destination is Amazon S3, Google Cloud Storage, Microsoft Azure or Sumo Logic. See [Developer documentation](https://developers.cloudflare.com/logs/logpush/logpush-configuration-api/understanding-logpush-api/#usage).
     */
    readonly ownershipChallenge: pulumi.Output<string | undefined>;
    /**
     * The zone identifier to target for the resource. Must provide only one of `accountId`, `zoneId`.
     */
    readonly zoneId: pulumi.Output<string | undefined>;
    /**
     * Create a LogpushJob resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: LogpushJobArgs, opts?: pulumi.CustomResourceOptions);
}
/**
 * Input properties used for looking up and filtering LogpushJob resources.
 */
export interface LogpushJobState {
    /**
     * The account identifier to target for the resource. Must provide only one of `accountId`, `zoneId`.
     */
    accountId?: pulumi.Input<string>;
    /**
     * The kind of the dataset to use with the logpush job. Available values: `accessRequests`, `casbFindings`, `firewallEvents`, `httpRequests`, `spectrumEvents`, `nelReports`, `auditLogs`, `gatewayDns`, `gatewayHttp`, `gatewayNetwork`, `dnsLogs`, `networkAnalyticsLogs`, `workersTraceEvents`, `devicePostureResults`, `zeroTrustNetworkSessions`, `magicIdsDetections`, `pageShieldEvents`.
     */
    dataset?: pulumi.Input<string>;
    /**
     * Uniquely identifies a resource (such as an s3 bucket) where data will be pushed. Additional configuration parameters supported by the destination may be included. See [Logpush destination documentation](https://developers.cloudflare.com/logs/reference/logpush-api-configuration#destination).
     */
    destinationConf?: pulumi.Input<string>;
    /**
     * Whether to enable the job.
     */
    enabled?: pulumi.Input<boolean>;
    /**
     * Use filters to select the events to include and/or remove from your logs. For more information, refer to [Filters](https://developers.cloudflare.com/logs/reference/logpush-api-configuration/filters/).
     */
    filter?: pulumi.Input<string>;
    /**
     * A higher frequency will result in logs being pushed on faster with smaller files. `low` frequency will push logs less often with larger files. Available values: `high`, `low`. Defaults to `high`.
     */
    frequency?: pulumi.Input<string>;
    /**
     * The kind of logpush job to create. Available values: `edge`, `instant-logs`, `""`.
     */
    kind?: pulumi.Input<string>;
    /**
     * Configuration string for the Logshare API. It specifies things like requested fields and timestamp formats. See [Logpush options documentation](https://developers.cloudflare.com/logs/logpush/logpush-configuration-api/understanding-logpush-api/#options).
     */
    logpullOptions?: pulumi.Input<string>;
    /**
     * The maximum uncompressed file size of a batch of logs. Value must be between 5MB and 1GB.
     */
    maxUploadBytes?: pulumi.Input<number>;
    /**
     * The maximum interval in seconds for log batches. Value must be between 30 and 300.
     */
    maxUploadIntervalSeconds?: pulumi.Input<number>;
    /**
     * The maximum number of log lines per batch. Value must be between 1000 and 1,000,000.
     */
    maxUploadRecords?: pulumi.Input<number>;
    /**
     * The name of the logpush job to create.
     */
    name?: pulumi.Input<string>;
    /**
     * Structured replacement for logpull*options. When including this field, the logpull*option field will be ignored.
     */
    outputOptions?: pulumi.Input<inputs.LogpushJobOutputOptions>;
    /**
     * Ownership challenge token to prove destination ownership, required when destination is Amazon S3, Google Cloud Storage, Microsoft Azure or Sumo Logic. See [Developer documentation](https://developers.cloudflare.com/logs/logpush/logpush-configuration-api/understanding-logpush-api/#usage).
     */
    ownershipChallenge?: pulumi.Input<string>;
    /**
     * The zone identifier to target for the resource. Must provide only one of `accountId`, `zoneId`.
     */
    zoneId?: pulumi.Input<string>;
}
/**
 * The set of arguments for constructing a LogpushJob resource.
 */
export interface LogpushJobArgs {
    /**
     * The account identifier to target for the resource. Must provide only one of `accountId`, `zoneId`.
     */
    accountId?: pulumi.Input<string>;
    /**
     * The kind of the dataset to use with the logpush job. Available values: `accessRequests`, `casbFindings`, `firewallEvents`, `httpRequests`, `spectrumEvents`, `nelReports`, `auditLogs`, `gatewayDns`, `gatewayHttp`, `gatewayNetwork`, `dnsLogs`, `networkAnalyticsLogs`, `workersTraceEvents`, `devicePostureResults`, `zeroTrustNetworkSessions`, `magicIdsDetections`, `pageShieldEvents`.
     */
    dataset: pulumi.Input<string>;
    /**
     * Uniquely identifies a resource (such as an s3 bucket) where data will be pushed. Additional configuration parameters supported by the destination may be included. See [Logpush destination documentation](https://developers.cloudflare.com/logs/reference/logpush-api-configuration#destination).
     */
    destinationConf: pulumi.Input<string>;
    /**
     * Whether to enable the job.
     */
    enabled?: pulumi.Input<boolean>;
    /**
     * Use filters to select the events to include and/or remove from your logs. For more information, refer to [Filters](https://developers.cloudflare.com/logs/reference/logpush-api-configuration/filters/).
     */
    filter?: pulumi.Input<string>;
    /**
     * A higher frequency will result in logs being pushed on faster with smaller files. `low` frequency will push logs less often with larger files. Available values: `high`, `low`. Defaults to `high`.
     */
    frequency?: pulumi.Input<string>;
    /**
     * The kind of logpush job to create. Available values: `edge`, `instant-logs`, `""`.
     */
    kind?: pulumi.Input<string>;
    /**
     * Configuration string for the Logshare API. It specifies things like requested fields and timestamp formats. See [Logpush options documentation](https://developers.cloudflare.com/logs/logpush/logpush-configuration-api/understanding-logpush-api/#options).
     */
    logpullOptions?: pulumi.Input<string>;
    /**
     * The maximum uncompressed file size of a batch of logs. Value must be between 5MB and 1GB.
     */
    maxUploadBytes?: pulumi.Input<number>;
    /**
     * The maximum interval in seconds for log batches. Value must be between 30 and 300.
     */
    maxUploadIntervalSeconds?: pulumi.Input<number>;
    /**
     * The maximum number of log lines per batch. Value must be between 1000 and 1,000,000.
     */
    maxUploadRecords?: pulumi.Input<number>;
    /**
     * The name of the logpush job to create.
     */
    name?: pulumi.Input<string>;
    /**
     * Structured replacement for logpull*options. When including this field, the logpull*option field will be ignored.
     */
    outputOptions?: pulumi.Input<inputs.LogpushJobOutputOptions>;
    /**
     * Ownership challenge token to prove destination ownership, required when destination is Amazon S3, Google Cloud Storage, Microsoft Azure or Sumo Logic. See [Developer documentation](https://developers.cloudflare.com/logs/logpush/logpush-configuration-api/understanding-logpush-api/#usage).
     */
    ownershipChallenge?: pulumi.Input<string>;
    /**
     * The zone identifier to target for the resource. Must provide only one of `accountId`, `zoneId`.
     */
    zoneId?: pulumi.Input<string>;
}
